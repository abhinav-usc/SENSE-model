# -*- coding: utf-8 -*-
"""PseudowordDatasetCreationAndAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18rhC9RdNemBpXXdtR7wT95bwWCXmedkd
"""

from google.colab import drive
import torch
import numpy as np
import torch.nn as nn
import pandas as pd
from transformers import RobertaTokenizer, RobertaModel
from transformers import BertTokenizer, BertModel
from transformers import AutoTokenizer
from torch.utils.data import DataLoader, TensorDataset, Dataset
import pickle
from sklearn.metrics.pairwise import cosine_similarity

drive.mount('/content/drive')

"""## Pseudoword Dataset create"""

# !pip install Wuggy

from wuggy import WuggyGenerator
g = WuggyGenerator()
g.load("orthographic_english")

df = pd.read_csv(
    '/content/drive/My Drive/Lancaster/english_words.txt',
    header=None,
    names=["word"],
    engine="python"
)

# Drop NaNs, ensure all are strings, strip whitespace, and lowercase
all_english_words = set(
    df["word"].dropna()          # remove NaN
      .astype(str)               # force string
      .str.strip()               # remove surrounding spaces
      .str.lower()               # lowercase
      .tolist()
)

print(len(all_english_words), "clean words loaded")

valid_words = []
oov_words = []

for w in map(str.lower, all_english_words):
    segs = g.lookup_reference_segments(w)
    if segs is None:
        oov_words.append(w)
    else:
        valid_words.append(w)

print(f"{len(valid_words)} in lexicon, {len(oov_words)} OOV")

# !pip install python-Levenshtein

import Levenshtein

tau = 0.85

pseudoword_df = pd.read_csv('/content/drive/My Drive/Lancaster/CSVs/wuggy_pseudowords_2.csv')

# remove all words from valid_words that are in pseudoword_df
valid_words = [word for word in valid_words if word not in pseudoword_df['word'].values]

print(len(valid_words))

# Precompute sets for O(1) checks
english_set = set(all_english_words)
existing_pseudoword_set = set(pseudoword_df['pseudoword']) if len(pseudoword_df) else set()


# Parameters
PROGRESS_EVERY = 200
FLUSH_EVERY = 5000   # how many accepted rows to batch before writing
OUTPUT = '/content/drive/My Drive/Lancaster/CSVs/wuggy_pseudowords_2.csv'


buffer_rows = []
accepted_since_flush = 0


for i,word in enumerate(valid_words):
    # light progress logging
    if i % PROGRESS_EVERY == 0:
        print(round((i / len(valid_words)) * 100, 2), "% done")

    # Generate candidate matches once, avoid per-row DataFrame
    matches = g.generate_classic([word])

    for match_found in matches:
      pseudoword = match_found['pseudoword']

      # filter word, step 1 Levenshtein
      if Levenshtein.ratio(word, pseudoword) > tau:
        print('skipping:', word, pseudoword)
        continue
      # step 2 ensure its a pseudoword
      if pseudoword in english_set:
        # print('skipping cuz real word', pseudoword)
        continue
      # step 3 ensure no duplicates
      if pseudoword in existing_pseudoword_set:
        # print('skipping cuz alr exists:', pseudoword)
        continue
      # add to buffer
      buffer_rows.append(match_found)
      existing_pseudoword_set.add(pseudoword)
      accepted_since_flush += 1
      if accepted_since_flush >= FLUSH_EVERY:
        pseudoword_df = pd.concat(
            [pseudoword_df, pd.DataFrame.from_records(buffer_rows)],
            ignore_index=True
        )
        pseudoword_df.to_csv(OUTPUT, index=False)
        buffer_rows.clear()
        accepted_since_flush = 0

# Final flush
if buffer_rows:
    pseudoword_df = pd.concat(
        [pseudoword_df, pd.DataFrame.from_records(buffer_rows)],
        ignore_index=True
    )

print(pseudoword_df.shape)
pseudoword_df.to_csv(OUTPUT, index=False)





"""## Construct CLS token dataset for all pseudowords generated"""

pseudoword_df = pd.read_csv('/content/drive/My Drive/Lancaster/CSVs/wuggy_pseudowords_2.csv')
pseudowords = pseudoword_df['pseudoword'].tolist()

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

# Tokenizeing all words and convert to IDs, padding/truncating as necessary
max_length = 20
tokens = tokenizer(pseudowords, return_tensors='pt', padding=True, truncation=True, max_length=max_length)

# Convert tokens to a DataLoader for efficient batching
batch_size = 32
dataset = TensorDataset(tokens['input_ids'], tokens['attention_mask'])
dataloader = DataLoader(dataset, batch_size=batch_size)

# Move model to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Pre-allocate tensor for [CLS] token representations
cls_tokens = torch.zeros((len(pseudowords), model.config.hidden_size), device=device)

# Process in batches
with torch.no_grad():
    model.eval()
    start_idx = 0
    for batch in dataloader:
        input_ids, attention_mask = [b.to(device) for b in batch]

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

        # Extract [CLS] tokens and store in pre-allocated tensor
        cls_representations = outputs.last_hidden_state[:, 0, :]
        cls_tokens[start_idx:start_idx+cls_representations.size(0)] = cls_representations

        start_idx += cls_representations.size(0)

"""### Neural Network"""

# creating the neural network
class Net(nn.Module):
    def __init__(self, d1, d2, embedding_type):
        super(Net, self).__init__()
        if embedding_type == 'word2vec' or embedding_type == 'word2vec_intersection':
          input_size = 300
        elif embedding_type == 'glove' or embedding_type == 'glove_intersection':
          input_size = 100
        elif embedding_type == 'bert' or embedding_type =='bert_bpe' or embedding_type == 'bert_intersection':
          input_size = 768
        elif embedding_type == 'bert_roberta':
          input_size = 768
        elif embedding_type == 'clip':
          input_size = 512
        elif embedding_type == 'blip':
          input_size = 768
        elif embedding_type == 'flan_t5':
          input_size = 2048
        elif embedding_type == 'sensorimotor_to_bert_cls':
          input_size = 11
        output_size = 11
        if embedding_type == 'sensorimotor_to_bert_cls':
          output_size = 768
        self.input_dim = input_size
        self.fc1 = nn.Linear(input_size, d1)  # input vector size is 300/100/768
        self.fc2 = nn.Linear(d1, d2)
        self.fc3 = nn.Linear(d2, output_size)  # assuming output vector size is 11 for sensorimotor
        self.act = nn.ReLU()
        self.final_act = nn.Sigmoid()  # or use clamp

    def forward(self, x):
        x = self.act(self.fc1(x))
        x = self.act(self.fc2(x))
        x = self.fc3(x)
        # x = self.final_act(x)  # Uncomment if Sigmoid activation is needed at the final layer
        # x = torch.clamp(x, min= 0, max=1)

        return x

data_type = 'bert'
PATH = "/content/drive/My Drive/Lancaster/NNmodel_bert.pt"
neural_model = Net(64, 128, data_type)
neural_model.load_state_dict(torch.load(PATH))

def batched_project(words, cls_tokens, neural_model, device=None, batch_size=256):
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    neural_model.to(device).eval()

    # Stack embeddings to a single [N, D] tensor
    embs = []
    for t in cls_tokens:
        t = torch.as_tensor(t)
        if t.ndim == 2 and t.size(0) == 1:
            t = t.squeeze(0)       # make it [D]
        assert t.ndim == 1, f"Expected [D], got {t.shape}"
        embs.append(t)
    X = torch.stack(embs, dim=0)   # [N, D]

    rows = []
    with torch.no_grad():
        for start in range(0, len(words), batch_size):
            if start % 100 == 0:
                print(f"{(start/len(words))*100}%")
            end = start + batch_size
            batch_words = words[start:end]
            batch_x = X[start:end].to(device)     # [B, D]

            # If your model expects [B, 1, D], use: preds = neural_model(batch_x.unsqueeze(1))
            preds = neural_model(batch_x)         # [B, M]
            preds_cpu = preds.cpu().numpy()
            batch_x_cpu = batch_x.cpu().numpy()

            for w, pred, emb in zip(batch_words, preds_cpu, batch_x_cpu):
                rows.append({
                    "word": w,
                    "sensorimotor": pred,         # numpy array for easy serialization
                    "embedding": emb              # numpy array
                })

    df = pd.DataFrame(rows)
    return df

# Use it
words = pseudowords
df = batched_project(words, cls_tokens, neural_model)

# Save
df.to_csv("/content/drive/My Drive/Lancaster/pseudowords_sensorimotor_projection.csv", index=False)

"""## Get maximized pseudowrds per modality"""

# df = pd.read_csv("/content/drive/My Drive/Lancaster/pseudowords_sensorimotor_projection.csv")


torch.serialization.add_safe_globals([pd.DataFrame])

df = torch.load(
    "/content/drive/My Drive/Lancaster/pseudowords_sensorimotor_projection.pt",
    weights_only=False
)

def get_top_pseudowords(modality_idx, k=10, device=None):
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    df_ = df

    sm_col = df_["sensorimotor"].tolist()
    if len(sm_col) == 0:
        return [], []

    first = sm_col[0]
    if isinstance(first, np.ndarray):
        sm_matrix = torch.from_numpy(np.stack(sm_col, axis=0))  # (V, D)
    elif torch.is_tensor(first):
        sm_matrix = torch.stack(sm_col, dim=0)                  # (V, D)
    else:
        # last resort, will work if sm_col is list-like of numbers
        sm_matrix = torch.as_tensor(sm_col)

    sm_matrix = sm_matrix.to(device=device, dtype=torch.float32)
    words = df_["word"].tolist()

    raw_vals = sm_matrix[:, modality_idx]
    probs = torch.sigmoid(raw_vals)

    k = min(k, probs.numel())
    vals, idx = probs.topk(k)              # vals: [k], idx: [k]
    words_top = [words[i] for i in idx.tolist()]
    distances = vals.tolist()
    return words_top, distances

modalities = ['Auditory', 'Gustatory',	'Haptic',
                        'Interoceptive',	'Olfactory',	'Visual',
                        'Foot_leg' ,	'Hand_arm',	'Head',
                        'Mouth', 'Torso']

top_pseudowords_per_modality = []

for i in range(11):
  print(modalities[i])
  w, d = get_top_pseudowords(i, k=500)
  top_pseudowords_per_modality.append(w)
  print(w)

# save
with open('/content/drive/My Drive/Lancaster/top_pseudowords_per_modality.pkl', 'wb') as f:
    pickle.dump(top_pseudowords_per_modality, f)



"""## Fliter pseudowords

Attempt to avoid words that differ from real words by only 1-2 characters
"""

# !pip install python-Levenshtein
import Levenshtein

# load
top_pseudowords_per_modality = []
modalities = ['Auditory', 'Gustatory',	'Haptic',
                        'Interoceptive',	'Olfactory',	'Visual',
                        'Foot_leg' ,	'Hand_arm',	'Head',
                        'Mouth', 'Torso']

with open('/content/drive/My Drive/Lancaster/top_pseudowords_per_modality.pkl', 'rb') as f:
    top_pseudowords_per_modality = pickle.load(f)

# Get all english words
df = pd.read_csv(
    '/content/drive/My Drive/Lancaster/english_words.txt',
    header=None,
    names=["word"],
    engine="python"
)

# Drop NaNs, ensure all are strings, strip whitespace, and lowercase
all_english_words = df["word"].dropna().astype(str).str.strip().str.lower().tolist()


filtered_pseudowords_per_modality = []

all_english_words = list(all_english_words)
tau = 0.85

for i, pseudoword_list in enumerate(top_pseudowords_per_modality):
  print(modalities[i])
  filtered_pseudowords = []
  for pseudo in pseudoword_list:
    # check all english words for similarity
    found = False
    for real_word in all_english_words:
      if Levenshtein.distance(pseudo, real_word) == 1:
        found = True
        break
    if not found:
      filtered_pseudowords.append(pseudo)
  filtered_pseudowords_per_modality.append(filtered_pseudowords)

for i, pseudoword_list in enumerate(filtered_pseudowords_per_modality):
  print(modalities[i])
  print(pseudoword_list)

Levenshtein.distance('wooblings', 'wobblings')

"""## Get Probabilities for the dataset"""

def get_modality_prob(modality_idx, words, device=None):

  max_length = 20
  tokens = tokenizer(words, return_tensors='pt', padding=True, truncation=True, max_length=max_length)

  # Convert tokens to a DataLoader for efficient batching
  batch_size = 32
  dataset = TensorDataset(tokens['input_ids'], tokens['attention_mask'])
  dataloader = DataLoader(dataset, batch_size=batch_size)

  # Move model to GPU if available
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model.to(device)

  # Pre-allocate tensor for [CLS] token representations
  cls_tokens = torch.zeros((len(words), model.config.hidden_size), device=device)

  # Process in batches
  with torch.no_grad():
      model.eval()
      start_idx = 0
      for batch in dataloader:
          input_ids, attention_mask = [b.to(device) for b in batch]

          outputs = model(input_ids=input_ids, attention_mask=attention_mask)

          # Extract [CLS] tokens and store in pre-allocated tensor
          cls_representations = outputs.last_hidden_state[:, 0, :]
          cls_tokens[start_idx:start_idx+cls_representations.size(0)] = cls_representations

          start_idx += cls_representations.size(0)

  neural_model.to(device).eval()

  # Stack embeddings to a single [N, D] tensor
  embs = []
  for t in cls_tokens:
      t = torch.as_tensor(t)
      if t.ndim == 2 and t.size(0) == 1:
          t = t.squeeze(0)       # make it [D]
      assert t.ndim == 1, f"Expected [D], got {t.shape}"
      embs.append(t)
  X = torch.stack(embs, dim=0)   # [N, D]

  rows = []
  with torch.no_grad():
      for start in range(0, len(words), batch_size):
          # if start % 100 == 0:
              # print(f"{(start/len(words))*100}%")
          end = start + batch_size
          batch_words = words[start:end]
          batch_x = X[start:end].to(device)     # [B, D]

          # If your model expects [B, 1, D], use: preds = neural_model(batch_x.unsqueeze(1))
          preds = neural_model(batch_x)         # [B, M]
          preds_cpu = preds.cpu().numpy()

          for w, pred in zip(batch_words, preds_cpu):
              rows.append({
                  "word": w,
                  "sensorimotor_mod": pred[modality_idx],         # numpy array for easy serialization
              })

  df = pd.DataFrame(rows)
  return df

import io

pseudo_words_selected = pd.read_csv(io.StringIO('''
crilollering,whebbling,squeping,croapings,stroblings,thellering,diturably,hularing,thrurring,squeanings,fluckles,thuckeding
susteries,ancechuttos,fifuffies,ascodiments,boincole,bambeagna,erpechitades,marreccone,aspintades,ipanelle,cazzagra,citocoli
sweadles,seasslins,pragingness,dabbedness,briagra,shomples,fleceils,corsenations,scalilla,flicetock,gormidion,nusprils
caeduseousness,fereniration,alphociration,grumbered,igmanilating,urpharities,frumbered,menaprigication,canmiobraine,incarpritation,operrhaging,squayophrenism
tygranate,myrticates,susteries,ascodiments,aspintades,tescourine,lusidour,bambeagna,selorine,brolorous,ancechuttos,erteycole
rehotes,scayons,ropsta,figels,diadegons,pylisation,acchots,erdellation,ardudellis,corsenations,margrel,figelled
forbicing,acribating,crettoes,raruting,ataulting,flompled,liechersedding,prebicing,trambings,crilders,covigating,refomped
sweadles,quiprigraphy,trilching,rechetsel,wrartering,pragingness,cacqueting,shoquets,concinging,encrocing,prynching,repufling
spaightness,obsightness,dispights,frumbered,recightlectly,chaspection,bicrinations,enrogiciously,grumbered,diruditions,sauging,risitating
fifuffies,fluckles,squeanings,ancechuttos,erpechitades,phongishments,bambeagna,boincole,squeasts,passagna,snially,conscrastions
tyvuniated,urpharities,fereniration,veysurition,fepupirition,saglidity,femiture,chrotutation,menniated,scuptitally,urreocatry,urerrivuration
'''), header=None)

pseudo_words_selected_per_modality = {}

modalities = ['Auditory', 'Gustatory',	'Haptic',
                        'Interoceptive',	'Olfactory',	'Visual',
                        'Foot_leg' ,	'Hand_arm',	'Head',
                        'Mouth', 'Torso']

for i, row in pseudo_words_selected.iterrows():
  pseudo_words_selected_per_modality[modalities[i]] = row.tolist()

import random

# Given top words per modality, returns 11 x 4 lists (11 modalities), each list with 7 words where 3 are from the modality and 4 distractors
def create_study_dataset(top_pseudowords_per_modality, num_distractors=4):
  modalities = ['Auditory', 'Gustatory',	'Haptic',
                        'Interoceptive',	'Olfactory',	'Visual',
                        'Foot_leg' ,	'Hand_arm',	'Head',
                        'Mouth', 'Torso']
  study_dataset = {} # format is 'modality': [[word_list],[word_list],[word_list],[word_list]]

  for modality_idx, modality in enumerate(modalities):
    print(modality)

    # get top words for this modality
    modality_words = top_pseudowords_per_modality[modality]
    # get their probabilities
    probs = get_modality_prob(modality_idx, modality_words)['sensorimotor_mod']
    # ensure all above 50%
    for prob in probs:
      if prob < 0.5:
        raise Exception('probs of top words not more than 50%')

    total_list = []
    all_distractors = []
    for i in range(0,12,3):
      # current word list with 3 correct words, will add 4 distractors
      word_list = modality_words[i:i+3]
      # Distractors, range of modality index
      range_lower = 0
      range_upper = 5 #keep range in perceptual by default
      if modality_idx > range_upper: #if action modality, change range
        range_lower = 6
        range_upper = 10

      # generate 4 random indices for modality of distractors
      distractors_indices = []
      for i in range(num_distractors):
        random_number  = modality_idx
        while random_number == modality_idx:
          random_number = random.randint(range_lower, range_upper)
        distractors_indices.append(random_number)
      # go to each of these modalities to pick a random word
      distractors = []
      for i in distractors_indices:
        top_words_for_modality = top_pseudowords_per_modality[modalities[i]]
        # choose random
        idx = 0
        # ensure prob < 0.5
        prob_for_word = 0.6
        timer = 0
        while prob_for_word > 0.5 or top_words_for_modality[idx] in all_distractors:
          if timer > 15:
            distractor_idx = i
            while distractor_idx == i or distractor_idx == modality_idx:
              distractor_idx = random.randint(range_lower, range_upper)
            top_words_for_modality = top_pseudowords_per_modality[modalities[distractor_idx]]
          idx = random.randint(0, len(top_words_for_modality)-1)
          prob_for_word = get_modality_prob(modality_idx, [top_words_for_modality[idx]])['sensorimotor_mod'][0]
          timer+=1

        distractors.append(top_words_for_modality[idx])
        all_distractors.append(top_words_for_modality[idx])
      word_list.extend(distractors)
      print(word_list)
      total_list.append(word_list)

    study_dataset[modality] = total_list
  return study_dataset

study_dataset = create_study_dataset(pseudo_words_selected_per_modality,num_distractors=4)

study_dataset_df = pd.DataFrame(study_dataset)
study_dataset_df.to_csv('/content/drive/My Drive/Lancaster/study_dataset.csv')

"""## Get sensorimotor prediction from model for each memeber of dataset"""

import ast
survey_dataset = pd.read_csv('/content/drive/My Drive/Lancaster/CSVs/survey_dataset.csv')
survey_dataset
survey_words_modality_probs_df = pd.DataFrame()

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
modalities = ['Auditory', 'Gustatory',	'Haptic',
                        'Interoceptive',	'Olfactory',	'Visual',
                        'Foot_leg' ,	'Hand_arm',	'Head',
                        'Mouth', 'Torso']

for mod_idx, modality in enumerate(modalities):
  all_questions = survey_dataset[modality].tolist()
  for index, question_words in enumerate(all_questions):
    question_words = ast.literal_eval(question_words)
    # each is a quetions, contains words
    sensorimotors = get_modality_prob(mod_idx, question_words)
    sensorimotors['modality'] = modality
    sensorimotors['question'] = modality + '.' + str(index+1)
    survey_words_modality_probs_df = pd.concat([survey_words_modality_probs_df, sensorimotors])

survey_words_modality_probs_df.to_csv('/content/drive/My Drive/Lancaster/CSVs/survey_words_modality_probs.csv')

"""## Sub-lexical Analysis of study data

### Split pseudowords per modality and get words per sublexical element
"""

survey_words_modality_probs_df = pd.read_csv('/content/drive/My Drive/Lancaster/CSVs/survey_words_modality_probs.csv')

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

max_length = 20
words = survey_words_modality_probs_df['word'].tolist()

all_modalities_sublexical_dist = []

# make sublexical components (character sets) from words
for mod_idx in range(0, 308, 28): # 11 modalities, 28 words each
  words = survey_words_modality_probs_df['word'].tolist()[mod_idx:mod_idx+28]
  sublexical_dist = {}
  min_len = 2
  max_len = 4
  for word in words:
    for char_set_len in range(min_len, max_len+1):
      for i in range(len(word)-char_set_len+1):
        cur_char_set = word[i:i+char_set_len]
        if not sublexical_dist or list(sublexical_dist.keys()).count(cur_char_set) == 0:
          sublexical_dist[cur_char_set] = []
        if word not in sublexical_dist[cur_char_set]:
          sublexical_dist[cur_char_set].append(word)
  all_modalities_sublexical_dist.append(sublexical_dist)


# save sublexical_dist
with open('/content/drive/My Drive/Lancaster/all_modalities_sublexical_dist.pkl', 'wb') as f:
  pickle.dump(all_modalities_sublexical_dist, f)
len(all_modalities_sublexical_dist)

with open('/content/drive/My Drive/Lancaster/all_modalities_sublexical_dist.pkl', 'rb') as f:
    all_modalities_sublexical_dist = pickle.load(f)
filtered_sublexical_dist_per_modality = []
for sublexical_dist in all_modalities_sublexical_dist:
  filtered_sublexical_dist = {}
  for char_set, words in sublexical_dist.items():
    if len(words) > 3: #~10% of 28
      filtered_sublexical_dist[char_set] = words
  updated_filtered_sublexical_dist = filtered_sublexical_dist.copy()
  # get rid of sublexical items that are contained within others and have the same words that contain it
  for cur_char_set, words_cur in filtered_sublexical_dist.items():
    for other_char_set, words_other in filtered_sublexical_dist.items():
      if not(other_char_set == cur_char_set) and other_char_set in cur_char_set and words_cur == words_other:
        if other_char_set in updated_filtered_sublexical_dist:
          updated_filtered_sublexical_dist.pop(other_char_set)
  filtered_sublexical_dist_per_modality.append(updated_filtered_sublexical_dist)

len(filtered_sublexical_dist_per_modality)

from collections import defaultdict

# Per modality, calculate P(w = m_L | c â†’ w)
# df, p(W-> M), P(W -> m| C1-> W ), P(W -> m| C2-> W ), P(W -> m| C3-> W ) ...

modalities = ['Auditory', 'Gustatory',	'Haptic',
                        'Interoceptive',	'Olfactory',	'Visual',
                        'Foot_leg' ,	'Hand_arm',	'Head',
                        'Mouth', 'Torso']


sublexical_word_probs_modality = []
study_data = pd.read_csv('/content/drive/My Drive/Lancaster/CSVs/final_with_sm.csv')

# Ensure rows for a modality are contiguous; remove if already true.
study_data = study_data.sort_values('modality')

cur_modality = None
cur_probs_total = []
cur_sublex_lists = defaultdict(list)

for _, row in study_data.iterrows():
    word = row['word']
    modality = row['modality']
    prob_mod = row['select_rate']

    if cur_modality is None:
        cur_modality = modality

    # If modality changes, finalize the previous one
    if modality != cur_modality:
        row_out = {
            'modality': cur_modality,
            'modality_word_prob': sum(cur_probs_total) / len(cur_probs_total),
            **{k: sum(v) / len(v) for k, v in cur_sublex_lists.items() if v}  # flatten and skip empties
        }
        sublexical_word_probs_modality.append(row_out)

        # reset accumulators
        cur_modality = modality
        cur_probs_total = []
        cur_sublex_lists = defaultdict(list)

    # accumulate
    cur_probs_total.append(prob_mod)
    mod_idx = modalities.index(modality)
    for char_set, words in filtered_sublexical_dist_per_modality[mod_idx].items():
        if word in words:
            cur_sublex_lists[char_set].append(prob_mod)

# flush the last modality
if cur_probs_total:
    row_out = {
        'modality': cur_modality,
        'modality_word_prob': sum(cur_probs_total) / len(cur_probs_total),
        **{k: sum(v) / len(v) for k, v in cur_sublex_lists.items() if v}
    }
    sublexical_word_probs_modality.append(row_out)

# save
sublexical_word_probs_modality_df = pd.DataFrame(sublexical_word_probs_modality)
sublexical_word_probs_modality_df.to_csv(
    '/content/drive/My Drive/Lancaster/CSVs/sublexical_words_modality.csv', index=False
)

sublexical_word_probs_modality_df

# get char sets for each mod for which prob is higher than avg prob
sublexical_word_probs_modality_df = pd.read_csv(
    '/content/drive/My Drive/Lancaster/CSVs/sublexical_words_modality.csv'
)
modality_char_sets = {}
for _, row in sublexical_word_probs_modality_df.iterrows():
    modality = row['modality']
    modality_word_prob = row['modality_word_prob']
    for char_idx in range(2,len(row)):
        char_set = sublexical_word_probs_modality_df.columns[char_idx]
        char_set_prob = row[char_set]
        if modality not in modality_char_sets:
            modality_char_sets[modality] = []
        modality_char_sets[modality].append((char_set, char_set_prob, row['modality_word_prob']))
for mod in modality_char_sets:
    modality_char_sets[mod].sort(key=lambda t: t[1], reverse=True)

# modality_char_sets is still available as a dict
# now convert it to a DataFrame
modalities = ['Auditory', 'Gustatory',	'Haptic',
                        'Interoceptive',	'Olfactory',	'Visual',
                        'Foot_leg' ,	'Hand_arm',	'Head',
                        'Mouth', 'Torso']
rows = []
for mod, tuples in modality_char_sets.items():
    for char_set, char_set_prob, modality_word_prob in tuples:
        model_rating_chars = get_modality_prob(modalities.index(mod), [char_set])['sensorimotor_mod'][0]
        rows.append({
            'modality': mod,
            'char_set': char_set,
            'char_set_prob': char_set_prob,
            'modality_word_prob': modality_word_prob,
            'model_rating_chars': model_rating_chars
        })

modality_char_sets_df = pd.DataFrame(rows)

# optional: quick sanity check in notebook
print(modality_char_sets_df.head())

# save to CSV
modality_char_sets_df.to_csv(
    '/content/drive/My Drive/Lancaster/CSVs/modality_char_sets.csv',
    index=False
)

"""### Plot"""

modality_char_sets_df = pd.read_csv(
    '/content/drive/My Drive/Lancaster/CSVs/modality_char_sets.csv'
)

import numpy as np
import matplotlib.pyplot as plt

# make sure this exists
modality_char_sets_df['char_minus_word'] = (
    modality_char_sets_df['char_set_prob'] - modality_char_sets_df['modality_word_prob']
)

r_vals = {}
for mod, group in modality_char_sets_df.groupby('modality'):
    x = group['char_minus_word'].values
    y = group['model_rating_chars'].values
    r_vals[mod] = np.corrcoef(x, y)[0, 1]
    plt.figure(figsize=(6, 4))

    # optional faint scatter so points are still visible behind text
    plt.scatter(x, y, alpha=0.0)  # invisible, just keeps axes sane

    # put the char_set text at each point
    for _, row in group.iterrows():
        plt.text(
            row['char_minus_word'],
            row['model_rating_chars'],
            row['char_set'],
            fontsize=8,
            ha='center',
            va='center'
        )


    plt.title(f'{mod}: P(Word in Modality | Char set in word) - P(Word in Modality) vs f(m_w)')
    plt.xlabel('P(Word in Modality | Char set in word) - P(Word in Modality)')
    plt.ylabel('P(f(m_w))')
    plt.tight_layout()
    plt.show()
# make r vals a table
r_vals_df = pd.DataFrame(list(r_vals.items()), columns=['modality', 'r'])
print(r_vals_df)